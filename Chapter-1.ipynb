{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1\n",
    "#### Notes\n",
    "* Main characteristics: trial and error search and delayed rewards\n",
    "* Formalize using the ideas from dynamical systems theory specifically as the optimal control of incompletely-known markov decision process.\n",
    "* MDP include sensation, action and goal in their simplest forms\n",
    "* RL main challenges arise from the trade-off between exploration and exploitation. \n",
    "* Main elements: a policy, a reward signal, a value function and a model of the environment\n",
    "* Policy: Association rule. It's just a mapping between the states and which action to take in that particular state. Or its the agent behavior at the given time.\n",
    "* Reward Signal: Defines the object of the problem we are trying to solve. In each step, the environment sends the agent a single value  called reward. The agent main objective is to maximize the total rewards in the long run. \n",
    "* Value function: Its just a numerical value for each state. Its an estimated rewards we will get if we follow the states in that order. Reward are immediate where as value functions are long-term. Prediction of rewards. Rewards are directly give by our environments where are values needs to be computed and predicted. Value estimation of uttermost importance.\n",
    "* Model of the environment are something that mimics the environment. Model-based RL is where planning is involved and Model-free RL is where only trial-error learning is done\n",
    "* State: It is the information avaliable to agent about the environment at a given time. \n",
    "* Most evolutionary, genetic programming, simulated annealing and other optimization methods search over multiple static policies with separate instance of the environment.\n",
    "* Simple value iteration for Tick-Tak-Toe :\n",
    "* $V_{S_t} \\leftarrow V_{S_t} + \\alpha (V_{S_{t+1}} - V_{S_t})%0$\n",
    "* Above equation is the basis for Temporal difference learning since it uses the difference between values to learn with the step size.\n",
    "\n",
    "#### Exercies:\n",
    "* 1.1 Self-Play: I think the algorithm will learn but with the slower rate since it is playing against itself. Since in the earlier part of the learning phase, the algorithm don’t know the right actions for each state, both the players would be just exploring so it would take longer for the RL agent to learn the optimal policy.\n",
    "\n",
    "* 1.2 Symmetries: I think embedding the symmetries of the game onto the learning algorithm will result in a biased algorithm i.e the algorithm might perform well on tick-tack-toe but will be worse for other games. Also, if the agent learns the symmetries by itself than its best but we providing the symmetries means we need to work on our part to find and exploit the symmetries. I don’t think the symmetrically equivalent positions should have the same values. Though symmetrical, the next action could result in different values. \n",
    "\n",
    "* 1.3 Greedy Plan: Generally, an epsilon greedy policy is applied so at to give some room for exploration. If will use a full greedy policy,there will not no exploration just exploitation. There are chances that it could perform worse than its nongreedy player.\n",
    "\n",
    "* 1.4 Learning for Exploration:  We will only update after all moves that the agent won’t have the direct action-consequences values so it will rather look like evolutionary algorithm solving the problem. What are the two sets of probability we learn?\n",
    "\n",
    "* 1.5 Improvements: Use Monte carlo search (MCS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
