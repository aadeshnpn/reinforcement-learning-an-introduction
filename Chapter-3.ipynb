{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes\n",
    "\n",
    "## Introduction\n",
    "* Markov Decision processes are derived from Markov Chains. Markov chains are like state machine transisting from one state to other in a probablistic fashion. Markov chains are memoryless i.e in a given state, its has the complete information to transition to next step don't need any history. MDP are the extension to markov chains where we can make decison/take action and MDP are classic fromalization of sequential decision making.\n",
    "* In bandit problem, we estimate $q_*(a)$ for each action, where as in MDP we will estimate $q_*(s,a)$ for each action a in each state s. $q_*(s,a)$ is the optimal policy. We can also estimate $v_*(s)$ for every state which is the optimal value function. See how the optimal policy function and optimal value function are related to state. This dependency to state allows for tradeoff between immediate and delayed reward.\n",
    "* MDP embedes a framework for interacting with the system to achieve a goal. The interacting entity is the agent, everything except agent is environment, based on the interaction the environment might give a return values defined as rewards. The primiry objective of the agent is to maximize the reward from the environment through its choice of action\n",
    "* Actions: $A_t \\in A(s)^3$, States: $S_t \\in S$, Rewards: $R_{t+1} \\in R \\subset \\mathbb{R}$\n",
    "* For a finite MDP, states, actions and rewards are finite so State and rewards has a probability distribution. Since MDP follows the markovian property, the probability distribution for a give state is only dependent on the previous state and action\n",
    "* i.e $p(s',r|s, a) \\dot = Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}$\n",
    "* The function $p$ is the dynamics of the MDP. From the theory of probability, $\\sum_{s' \\in S}\\sum_{r \\in R} p(s',r|s,a)=1$ for all $s \\in S$, $a \\in A(s)$, this is to say that, the probabilty of taking all possible action is a particular state should be 1.\n",
    "* Given the dynamics $p$, we can compute the state transition probabilities, $p(s'|s,a) \\dot = Pr\\{S_t=s'|S_{t-1}=s,A_{t-1}=a\\} = \\sum_{r \\in R} p(s', r|s,a)$\n",
    "* Similary, we can compute expected rewards for the state-action pairs $r(s, a) \\dot = \\mathbb{E}[R_t|S_{t-1}=s, A_{t-1}=a] = \\sum_{r \\in R}r \\sum s'_{s} p(s', r|s,a)$\n",
    "* MDP framework flexiable and abstract so making it useful to solve problems in macro and mini sizes. \n",
    "* Discussion about agent and envionment boundries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
