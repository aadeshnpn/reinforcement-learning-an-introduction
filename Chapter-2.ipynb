{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Chapter 2 Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Points\n",
    "* True value of the action is the mean reward when the action is taken overtime\n",
    "* True value can be found using the expectation formula\n",
    "* For k-armed bandit problem, each K actions has an expected/mean reward := Value of the action ($q_*(a)$)\n",
    "* $q_*(a) =  E [R_t|A_t=a]$, is the value but in most of the cases we don't know this value\n",
    "* So we use Action-Value methods <- Esitmates the value of the action and use it to take decision.\n",
    "* $Q_t(a)=\\frac{\\sum_{i=1}^{t-1} R_i \\cdot 1_{A_i=a}}{\\sum_{i=1}^{t-1}1_{A_i=a}}$\n",
    "* Alternatively, $Q_{n+1} = \\frac{\\sum_{i=1}^{n} R_i}{n}$\n",
    "* Now based on the $Q_t(a)$ at time step t, we can choose the best action as $A_t =  argmax_a Q_t(a)$\n",
    "* We can expand the alternative defination of expected value as $Q_{n+1} = Q_n + \\frac{R_n-Q_n}{n}$\n",
    "* In general form, newEstimate <- oldEstimate + stepSize [Target - oldEstimate]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Bandit Algorithm Pseudocode\n",
    "* For all actions, initialize $Q(a)\\leftarrow 0, N(a)\\leftarrow 0$ where N(a) is the number of times the action a has been selected\n",
    "* While True:\n",
    "    + $A \\leftarrow argmax_a Q(a)$ with probability 1-$\\epsilon$ or a random action with probability $\\epsilon$\n",
    "    + $R \\leftarrow bandit(A)$ // Reward signal from the environment/bandit\n",
    "    + $N(A)\\leftarrow N(A) + 1$\n",
    "    + $Q(A)\\leftarrow Q(A) + \\frac{R - Q(A)}{N(A)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value Estimate [ 3.31432096 12.11662729  9.85018956 17.07896357  6.75562677  0.\n",
      "  0.          4.85355053 19.94358169 -3.13060835]\n",
      "Action count [   2.    8.    4.    2.    8.    0.    0.  107. 1866.    3.]\n",
      "[(5, 10), (10, 10), (10, 1), (5, 10), (10, 20), (5, 20), (5, 10), (5, 1), (20, 10), (1, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Simple bandit algorithm for k = 10 based on the Pseudo code above\n",
    "# Import statement\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the random state for consistent results\n",
    "random = np.random.RandomState(123)\n",
    "\n",
    "# Bandit class which a normal distribution\n",
    "class Bandit:\n",
    "    # Initialize\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        # self.sd = random.randint(1,5,1)[0]\n",
    "        self.sd = random.choice([1, 5, 10, 20])\n",
    "        self.mean = random.choice([1, 5, 10, 20])\n",
    "    \n",
    "    # For printing the object\n",
    "    #def __repr__(self):\n",
    "    #    print (self.mean)\n",
    "    #    print (self.sd)\n",
    "    \n",
    "    # Generate reward for playing this bandit    \n",
    "    def step(self):\n",
    "        return random.normal(self.mean, self.sd, 1)[0]\n",
    "\n",
    "\n",
    "\n",
    "# Define k\n",
    "k = 10\n",
    "\n",
    "# Initialize numpy array of Q and N\n",
    "q_a = np.zeros(k)\n",
    "n_a = np.zeros(k)\n",
    "\n",
    "# Define epsilon\n",
    "epsilon = 0.01\n",
    "\n",
    "# Construct k bandits\n",
    "bandits = []\n",
    "for i in range(k):\n",
    "    bandits.append(Bandit(i))\n",
    "\n",
    "# Main loop    \n",
    "for step in range(2000):\n",
    "    # Explotitation\n",
    "    if random.rand() < (1 - epsilon):\n",
    "        indx = np.argmax(q_a)\n",
    "        \n",
    "        # Check if there are multiple max values\n",
    "        results = q_a[:]==q_a[indx]\n",
    "        \n",
    "        # If there are multiple max values choose the max arg at random\n",
    "        if np.sum(results*1)>1:\n",
    "            action = random.choice(np.argwhere(results==True).flatten())\n",
    "        else:\n",
    "            action = indx\n",
    "    \n",
    "    # Exploration\n",
    "    else:\n",
    "        # Make the action choice at random\n",
    "        action = random.choice(range(0,k))\n",
    "    \n",
    "    # Get the reward from bandit for playing that action\n",
    "    reward = bandits[action].step()\n",
    "    \n",
    "    # Increment the N_A array for taking that action\n",
    "    n_a[action] += 1\n",
    "    \n",
    "    # Update the Q_A / estimate in incremental fashion\n",
    "    # newEstimate <- oldEstimate + stepSize * [Target - oldEstimate]\n",
    "    q_a[action] = q_a[action] + (reward - q_a[action])/n_a[action]\n",
    "        \n",
    "# Print the value of the array to see if everything looks good        \n",
    "print ('Mean Value Estimate', q_a)\n",
    "print ('Action count', n_a)\n",
    "print ([(bandit.mean,bandit.sd) for bandit in bandits])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple bandit result discussion\n",
    "The simple bandit algorithm did the correct choice by choosing the banding with mean 20 and standard deviation of 10 because it can gain the highest reward by playing this bandit. Furthermore, the mean value estimates shows the value estimate of 19.94 which is almost equal to the real mean value of 20. Thus, simple value exploitation did pretty good on this k-arm bandit task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* The simple bandit algorithm based on Average Value is suitable for stationary bandits i.e where the rewards probability density function don't change.\n",
    "* One of the popular ways to handle non-stationary bandits is to introduce constant step size instead of 1/n. \n",
    "* Using this constant in the formula, gives us a weighted average formula, $Q_{n+1}= (1-\\alpha)^n Q_1 + \\sum_{i=1}^{n}\\alpha (1-\\alpha)^{n-i}R_i$\n",
    "* This formula is sometimes also called exponential recency-weighted average\n",
    "* The cool thing is that the sum of the weights equals to 1 and the weight given to reward R_i depends on how many reswards ago n-1 was seen. \n",
    "* We can choose different values of step size. When step size = 1/n then its the sample-average method and it is guaranteed to converge. But not all the values of step size will converge. So the choice of step size is important. \n",
    "* The conditions for the convergence are $\\sum_{n=1}^{\\infty} \\alpha_n(a) = \\infty$ and $\\sum_{n=1}^{\\infty} \\alpha_n^2(a) < \\infty$\n",
    "* The first conditions says that the steps are large enough to overcome random fluctions and the second condition ensures that the step are small enough to assure convergence.\n",
    "\n",
    "### Exercise 2.4\n",
    "If the step-size parameters are not constant, then the estimate Q_n is will a bit bit different than the equation we saw before. Instead of using the same alpha every time, we need to use the value of alpha from the alpha list in each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5\n",
    "Experiment for various sample-average methods for non-stationary problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value Estimate [  53.40760137   40.03305054   62.92512607   48.98140916   50.30017832\n",
      "   25.61019974   49.1277202  4580.40930063   75.52259888   73.06814855]\n",
      "Action count [ 115.   84.   92.   87.   92.   85.   83. 9149.  105.  108.]\n",
      "[(5, 10), (5, 10), (10, 20), (5, 1), (20, 10), (5, 20), (10, 20), (5, 1), (20, 20), (20, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Simple bandit algorithm for k = 10 based on the Pseudo code above\n",
    "# Import statement\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the random state for consistent results\n",
    "random = np.random.RandomState(123)\n",
    "\n",
    "# Define no.of trails\n",
    "trails = 10000\n",
    "\n",
    "# Bandit class which a normal distribution\n",
    "class Bandit:\n",
    "    # Initialize\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        # self.sd = random.randint(1,5,1)[0]\n",
    "        self.sd = random.choice([1, 5, 10, 20])\n",
    "        self.mean = random.choice([1, 5, 10, 20])\n",
    "        self.reward = random.normal(self.mean, self.sd, 1)[0]\n",
    "    \n",
    "    # For printing the object\n",
    "    #def __repr__(self):\n",
    "    #    print (self.mean)\n",
    "    #    print (self.sd)\n",
    "    \n",
    "    # Generate reward for playing this bandit    \n",
    "    def step(self):\n",
    "        # As the exercise suggest, we need to add a noise of mean 1 and sd 0.01\n",
    "        noise = random.normal(1, 0.01, 1)[0]\n",
    "        self.reward += noise\n",
    "        return self.reward\n",
    "\n",
    "\n",
    "# Define k\n",
    "k = 10\n",
    "\n",
    "# Initialize numpy array of Q and N\n",
    "q_a = np.zeros(k)\n",
    "n_a = np.zeros(k)\n",
    "\n",
    "# Define epsilon\n",
    "epsilon = 0.1\n",
    "\n",
    "# Define alpha/step size\n",
    "alpha = 0.1\n",
    "\n",
    "# Define which method to use for update\n",
    "method = 'I' # Options: I <- incremental, C <- constant step size\n",
    "\n",
    "# Construct k bandits\n",
    "bandits = []\n",
    "for i in range(k):\n",
    "    bandits.append(Bandit(i))\n",
    "\n",
    "# Make a list to store the rewards for 8th arm\n",
    "reward_store = []\n",
    "# Main loop    \n",
    "for step in range(trails):\n",
    "    # Explotitation\n",
    "    if random.rand() < (1 - epsilon):\n",
    "        indx = np.argmax(q_a)\n",
    "        \n",
    "        # Check if there are multiple max values\n",
    "        results = q_a[:]==q_a[indx]\n",
    "        \n",
    "        # If there are multiple max values choose the max arg at random\n",
    "        if np.sum(results*1)>1:\n",
    "            action = random.choice(np.argwhere(results==True).flatten())\n",
    "        else:\n",
    "            action = indx\n",
    "    \n",
    "    # Exploration\n",
    "    else:\n",
    "        # Make the action choice at random\n",
    "        action = random.choice(range(0,k))\n",
    "    \n",
    "    # Get the reward from bandit for playing that action\n",
    "    reward = bandits[action].step()\n",
    "    \n",
    "    # Increment the N_A array for taking that action\n",
    "    n_a[action] += 1\n",
    "    \n",
    "    # Update the Q_A / estimate in incremental fashion\n",
    "    # newEstimate <- oldEstimate + stepSize * [Target - oldEstimate]\n",
    "    if method == 'I':\n",
    "        q_a[action] = q_a[action] + (reward - q_a[action])/n_a[action]\n",
    "    elif method == 'C':\n",
    "        # Update using constant step-size\n",
    "        q_a[action] = q_a[action] + (reward - q_a[action]) * alpha\n",
    "    else:\n",
    "        # No method selected. So no updated\n",
    "        pass\n",
    "    \n",
    "    reward_store.append(np.mean(q_a))\n",
    "    \n",
    "reward_store = np.array(reward_store)\n",
    "plt.plot(list(range(trails)), reward_store)\n",
    "plt.show()\n",
    "# Print the value of the array to see if everything looks good        \n",
    "print ('Mean Value Estimate', q_a)\n",
    "print ('Action count', n_a)\n",
    "print ([(bandit.mean,bandit.sd) for bandit in bandits])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* As we have noticed, we need to have an initial estimate of Q_a. For above examples we set it to zero but sometimes it can be used to inject the knowledge the human already have about the system\n",
    "* Also the book gives a awesome example of the use of initial Q_a values to explore more. At first, Q_a is set to 5. But the mean reward is 1 so the algoritm is forced to explore quite a bit before converging to the mean value. Thus the choice of initial values can be used for some tricks and optimisim. \n",
    "\n",
    "### Exercise 2.6\n",
    "At start there are lots of spikes as the resward is set to 5, so the greedy method tries to exploit but next times ends up exploring. This method performs better as all the k bandits are at least choosen once. In normal case, if one of the bandits are never explored, the performance suffers. So this method is better than simple method due to extensive exploration\n",
    "\n",
    "### Exercise 2.7\n",
    "Mathametical analysis done in paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* Exploration is need so as to check if there are actions which are leftout but could be optimal. Using greedy approach we don't do exploration. With epsilon greedy we do somekind of exploration but still we focus on exploitation. It would be good if we could explore and rank the explored options based on their potential. \n",
    "* Such a method is upper-confidence-bound action selection and its given by $A_t = argmax_a \\big[ Q_t(a) + c  \\sqrt\\frac{\\ln t}{N_t(a)} \\big]$\n",
    "* Upper Confidence Bound (UCB) is that the second term (square root) is the representation of uncertainty. As we use the action a, the denominator N_t(a) increases, thus reducing the uncertainy in the estimate.\n",
    "\n",
    "### Exercise 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
