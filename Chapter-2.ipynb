{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Chapter 2 Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Points\n",
    "* True value of the action is the mean reward when the action is taken overtime\n",
    "* True value can be found using the expectation formula\n",
    "* For k-armed bandit problem, each K actions has an expected/mean reward := Value of the action ($q_*(a)$)\n",
    "* $q_*(a) =  E [R_t|A_t=a]$, is the value but in most of the cases we don't know this value\n",
    "* So we use Action-Value methods <- Esitmates the value of the action and use it to take decision.\n",
    "* $Q_t(a)=\\frac{\\sum_{i=1}^{t-1} R_i \\cdot 1_{A_i=a}}{\\sum_{i=1}^{t-1}1_{A_i=a}}$\n",
    "* Alternatively, $Q_{n+1} = \\frac{\\sum_{i=1}^{n} R_i}{n}$\n",
    "* Now based on the $Q_t(a)$ at time step t, we can choose the best action as $A_t =  argmax_a Q_t(a)$\n",
    "* We can expand the alternative defination of expected value as $Q_{n+1} = Q_n + \\frac{R_n-Q_n}{n}$\n",
    "* In general form, newEstimate <- oldEstimate + stepSize [Target - oldEstimate]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Bandit Algorithm Pseudocode\n",
    "* For all actions, initialize $Q(a)\\leftarrow 0, N(a)\\leftarrow 0$ where N(a) is the number of times the action a has been selected\n",
    "* While True:\n",
    "    + $A \\leftarrow argmax_a Q(a)$ with probability 1-$\\epsilon$ or a random action with probability $\\epsilon$\n",
    "    + $R \\leftarrow bandit(A)$ // Reward signal from the environment/bandit\n",
    "    + $N(A)\\leftarrow N(A) + 1$\n",
    "    + $Q(A)\\leftarrow Q(A) + \\frac{R - Q(A)}{N(A)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value Estimate [ 3.31432096 12.11662729  9.85018956 17.07896357  6.75562677  0.\n",
      "  0.          4.85355053 19.94358169 -3.13060835]\n",
      "Action count [   2.    8.    4.    2.    8.    0.    0.  107. 1866.    3.]\n",
      "[(5, 10), (10, 10), (10, 1), (5, 10), (10, 20), (5, 20), (5, 10), (5, 1), (20, 10), (1, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Simple bandit algorithm for k = 10 based on the Pseudo code above\n",
    "# Import statement\n",
    "import numpy as np\n",
    "\n",
    "# Define the random state for consistent results\n",
    "random = np.random.RandomState(123)\n",
    "\n",
    "# Bandit class which a normal distribution\n",
    "class Bandit:\n",
    "    # Initialize\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        # self.sd = random.randint(1,5,1)[0]\n",
    "        self.sd = random.choice([1, 5, 10, 20])\n",
    "        self.mean = random.choice([1, 5, 10, 20])\n",
    "    \n",
    "    # For printing the object\n",
    "    #def __repr__(self):\n",
    "    #    print (self.mean)\n",
    "    #    print (self.sd)\n",
    "    \n",
    "    # Generate reward for playing this bandit    \n",
    "    def step(self):\n",
    "        return random.normal(self.mean, self.sd, 1)[0]\n",
    "\n",
    "\n",
    "\n",
    "# Define k\n",
    "k = 10\n",
    "\n",
    "# Initialize numpy array of Q and N\n",
    "q_a = np.zeros(k)\n",
    "n_a = np.zeros(k)\n",
    "\n",
    "# Define epsilon\n",
    "epsilon = 0.01\n",
    "\n",
    "# Construct k bandits\n",
    "bandits = []\n",
    "for i in range(k):\n",
    "    bandits.append(Bandit(i))\n",
    "\n",
    "# Main loop    \n",
    "for step in range(2000):\n",
    "    # Explotitation\n",
    "    if random.rand() < (1 - epsilon):\n",
    "        indx = np.argmax(q_a)\n",
    "        \n",
    "        # Check if there are multiple max values\n",
    "        results = q_a[:]==q_a[indx]\n",
    "        \n",
    "        # If there are multiple max values choose the max arg at random\n",
    "        if np.sum(results*1)>1:\n",
    "            action = random.choice(np.argwhere(results==True).flatten())\n",
    "        else:\n",
    "            action = indx\n",
    "    \n",
    "    # Exploration\n",
    "    else:\n",
    "        # Make the action choice at random\n",
    "        action = random.choice(range(0,k))\n",
    "    \n",
    "    # Get the reward from bandit for playing that action\n",
    "    reward = bandits[action].step()\n",
    "    \n",
    "    # Increment the N_A array for taking that action\n",
    "    n_a[action] += 1\n",
    "    \n",
    "    # Update the Q_A / estimate in incremental fashion\n",
    "    # newEstimate <- oldEstimate + stepSize * [Target - oldEstimate]\n",
    "    q_a[action] = q_a[action] + (reward - q_a[action])/n_a[action]\n",
    "        \n",
    "# Print the value of the array to see if everything looks good        \n",
    "print ('Mean Value Estimate', q_a)\n",
    "print ('Action count', n_a)\n",
    "print ([(bandit.mean,bandit.sd) for bandit in bandits])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple bandit result discussion\n",
    "The simple bandit algorithm did the correct choice by choosing the banding with mean 20 and standard deviation of 10 because it can gain the highest reward by playing this bandit. Furthermore, the mean value estimates shows the value estimate of 19.94 which is almost equal to the real mean value of 20. Thus, simple value exploitation did pretty good on this k-arm bandit task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
