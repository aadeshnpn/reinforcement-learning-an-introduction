{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes\n",
    "\n",
    "## Introduction\n",
    "* Markov Decision processes are derived from Markov Chains. Markov chains are like state machine transisting from one state to other in a probablistic fashion. Markov chains are memoryless i.e in a given state, its has the complete information to transition to next step don't need any history. MDP are the extension to markov chains where we can make decison/take action and MDP are classic fromalization of sequential decision making.\n",
    "* In bandit problem, we estimate $q_*(a)$ for each action, where as in MDP we will estimate $q_*(s,a)$ for each action a in each state s. $q_*(s,a)$ is the optimal policy. We can also estimate $v_*(s)$ for every state which is the optimal value function. See how the optimal policy function and optimal value function are related to state. This dependency to state allows for tradeoff between immediate and delayed reward.\n",
    "* MDP embedes a framework for interacting with the system to achieve a goal. The interacting entity is the agent, everything except agent is environment, based on the interaction the environment might give a return values defined as rewards. The primiry objective of the agent is to maximize the reward from the environment through its choice of action\n",
    "* Actions: $A_t \\in A(s)^3$, States: $S_t \\in S$, Rewards: $R_{t+1} \\in R \\subset \\mathbb{R}$\n",
    "* For a finite MDP, states, actions and rewards are finite so State and rewards has a probability distribution. Since MDP follows the markovian property, the probability distribution for a give state is only dependent on the previous state and action\n",
    "* i.e $p(s',r|s, a) \\dot = Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\}$\n",
    "* The function $p$ is the dynamics of the MDP. From the theory of probability, $\\sum_{s' \\in S}\\sum_{r \\in R} p(s',r|s,a)=1$ for all $s \\in S$, $a \\in A(s)$, this is to say that, the probabilty of taking all possible action is a particular state should be 1.\n",
    "* Given the dynamics $p$, we can compute the state transition probabilities, $p(s'|s,a) \\dot = Pr\\{S_t=s'|S_{t-1}=s,A_{t-1}=a\\} = \\sum_{r \\in R} p(s', r|s,a)$\n",
    "* Similary, we can compute expected rewards for the state-action pairs $r(s, a) \\dot = \\mathbb{E}[R_t|S_{t-1}=s, A_{t-1}=a] = \\sum_{r \\in R}r \\sum s'_{s} p(s', r|s,a)$\n",
    "* MDP framework flexiable and abstract so making it useful to solve problems in macro and mini sizes. \n",
    "* Discussion about agent and envionment boundries\n",
    "* Different examples on agent/environement for Bioreactors, Pick-and-Place Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "* State machine for a light bulb\n",
    "    + States: On/Off, action: button flip, Rewards: Light\n",
    "* Chatbot\n",
    "    + States: All vocublary set, action: word/sentence, Rewards: User engagement time\n",
    "    \n",
    "* Cooking\n",
    "    + States: All the ingredients, action: amount of heat, Rewards: Tasty food\n",
    "    \n",
    "### Exercise 3.2\n",
    "* MDP framework is quite useful when the environment can easily represented by state and there is a clear boundary between what parts of environment is agent and what are not. Sometimes in realworld problem its hard to identify the boundary\n",
    "\n",
    "### Exercise 3.3\n",
    "* In problem of driving, its hard to say that a line is clear winner. It all depends on which actions are easy to measure and use it to get towards the goal. Its lots easier to measure the steering wheel angle, accleration of the car and its velocity so it makes lot more sense to take these as action. Whereas its lot harded to accruately measure the muscle tension and its lot harded to get the human intent and theri intention. So, whatever actions is easy measured in the environment could be a metric for its usages\n",
    " \n",
    "### Exercise 3.4\n",
    "* Finding it hard to draw table in markdown, so doing this assignment in actual notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals and Rewards\n",
    "* Reward signal formalized the notion of goal and the agent task in RL is to have large total number of rewards\n",
    "* Designing reward signal is an art\n",
    "* If $G_t$ is the expected return, then $G_t = R_{t+1} + R_{t+2} + R_{t+3}+...+R_T$, where T is the final time step\n",
    "* When the agent-environment interaction can be easily truned into sub-sequence then these interaction can be dividied into episodes. Generally in games, the episodes ends in a terminal state and the states of the environment being reset into initial states\n",
    "* In many cases, the interaction can't be turned into episodes, which is continuing tasks. In this case $T=\\infty$, the final reward is hard to identify. To deal with this infinity, the term discounting is introduced. What this means is that, the immediate rewards are more valuable than the future rewards. Mathematically,\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+2} + \\gamma^3R_{t+3}... = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}$. $\\gamma$ is called the discount rate and $\\gamma \\in [0,1]$\n",
    "* So if $\\gamma=0$ then the $G_t$ depends only on the immediate rewards if the its value is greater than 0, then some importance is given to the future rewards as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
